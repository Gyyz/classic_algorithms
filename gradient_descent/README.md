# Gradient Descent

Gradient descent iteratively updates parameters in the direction of the negative gradient to minimize a function.

This demo minimizes `f(x) = (x-3)^2 + 1`.

Key ideas:

- Step size (learning rate) controls progress.
- Stop when updates are small or max iterations reached.